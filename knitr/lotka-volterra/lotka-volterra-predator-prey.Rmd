---
title: "Estimating Lotka-Volterra Predator-Prey Dynamics with Stan"
author: "Bob Carpenter"
date: "October 16, 2017"
output:
  html_document:
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reshape)
library(ggplot2)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores(logical = FALSE))
```

<br />

<div style="width:3in; float:right; padding: 1em">
![Canadian lynx](Canadian_lynx_by_Keith_Williams.jpg)
Predator: *Canadian lynx*
<br /><span style="font-size:60%; padding: -1em 0 1em 0">&copy; 2009, Keith Williams, CC-BY 2.0</span>

<br style="padding:5em 0 0 0"/>

![Snowshoe hare](Snowshoe_Hare,_Shirleys_Bay.jpg)
Prey: *snowshoe hare*
<br /><span style="font-size:60%">&copy; 2013, D. Gordon E. Robinson, CC-BY SA 3.0</span>
</div>

#### Abstract

The Lotka-Volterra equations define parametric differential equations for the fluctuation of predator and prey populations.  To estimate the parameters of such a model, a model for measurement error and unexplained variation is layered on top of the deterministic dynamics.  The model is coded in Stan and fit to data on Canadian lynxes and snowshoe hares based on numbers of pelts collected in the early 20th century by the Hudson Bay Company.


## Lynxes and Hares, 1900-1920

The Hudson Bay Company recorded the number of captured pelts of two species between 1900 and 1920,

* [snowshoe hares](https://en.wikipedia.org/wiki/Snowshoe_hare), an hervivorous cousin of rabbits, and

* [Canadian lynxes](https://en.wikipedia.org/wiki/Canada_lynx), a feline predator whose diet consists almost exclusively of hares.

The date provided here was converted to comma-separated value (CSV) format from (Howard 2009).

```{r}
lynx_hare_df <-
  read.csv("hudson-bay-lynx-hare.csv", comment.char="#")
head(lynx_hare_df, n = 3)
```


The number of pelts taken by the Hudson Bay Company is shown over time as follows (first, the data is melted using the reshape package, then plotted by species using ggplot).

```{r}
lynx_hare_melted_df <- melt(as.matrix(lynx_hare_df[, 2:3]))
colnames(lynx_hare_melted_df) <- c("year", "species", "pelts")
lynx_hare_melted_df$year <-
  lynx_hare_melted_df$year +
  rep(1899, length(lynx_hare_melted_df$year))
head(lynx_hare_melted_df, n=3)
tail(lynx_hare_melted_df, n=3)
```
```{r}
population_plot2 <-
  ggplot(data = lynx_hare_melted_df,
         aes(x = year, y = pelts, color = species)) +
  geom_line() +
  geom_point() +
  ylab("pelts (thousands)")
population_plot2
```

This plot makes it clear that the spikes in the lynx population lag those in the hare population.  In both populations, the periodicity appears to be somewhere in the neighborhood of ten to twelve years.

Volterra (1926) plotted the temporal dynamics of predator and prey populations using an axis for each species and then plotting the temporal course as a line.  The result for the lynx and hare population is easily plotted from the original data frame.

```{r}
population_plot1 <-
  ggplot(data = lynx_hare_df,
         aes(x = Lynx, y = Hare, color = Year)) +
  geom_path() +
  geom_point() +
  xlab("lynx pelts (thousands)") +
  ylab("hare pelts (thousands)")
population_plot1
```

As can be seen from the diagram, the population dynamics orbit in an apparently stable pattern.


## The Lotka-Volterra Equations

The Lotka-Volterra equations (Volterra 1926, 1927; Lotka 1925) are based on the assumptions that

* the predator population intrinsically shrinks,

* the prey population intrinsically grows,

* larger prey population leads to larger predator population, and

* larger predator population leads to smaller prey populations.

Together, these dynamics lead to a cycle of rising and falling populations.  With a low lynx population, the hare population grows.  As the hare population grows, it allows the lynx population to grow.  Eventually, the lynx population is large enough to start cutting down on the hare population.  That in turn puts downward pressure on the lynx population.  The cycle then resumes from where it started.

The Lotka-Volterra equations (Volterra 1926, 1927; Lotka 1925) are a pair of first-order differential equations describing the population dynamics of a pair of species, one predator and one prey  Suppose that

* $u(t) \geq 0$ is the population size of the prey species at time $t$, and

* $v(t) \geq 0$ is the population size of the predator species.

Volterra modeled the temporal dynamics of the two species (i.e., population sizes over times) in terms of four parameters, $\alpha, \beta, \gamma, \delta > 0$, as
$$
\begin{eqnarray}
\frac{\mathrm{d}}{\mathrm{d}t} u
& = &  (\alpha - \beta v) u
& = & \alpha u - \beta u v
\\[6pt]
\frac{\mathrm{d}}{\mathrm{d}t} v
& = &  (-\gamma + \delta \, u) \, v
& = & -\gamma v + \delta uv
\end{eqnarray}
$$
As usual in writing differential equations, $u(t)$ and $v(t)$ are rendered as $u$ and $v$ to simplify notation.


## Error model: measurement and unexplained variation

The Lotka-Volterra model is deterministic.  Given the system parameters and the initial conditions, the population dynamics are fully determined.  We will specify a statistical model that allows us to infer the parameters of the model and predict future population dynamics based on noisy measurements and a model that does not explain all of the observed variation in the data.  We will consider two sources of error.

First, the theory is not expected to be that good in this case, so there will be resulting unexplained variation.  For example, the weather in a particular year is going to have an impact on the populations, but it is not taken into account, leading to variation that is not explained by the model.

The second source of error is noisy measurements of the population.  We cannot measure the population directly, so instead make do with noisy measurements, such as the number of pelts collected.  In more elaborate models (beyond what we consider here), measurements of pelts collected could be supplemented with output of other measurements, such as mark-recapture studies.

#### A linear regression analogy
Like a simple linear regression, or non-linear GLM, the trick is to treat the underlying determinstic model as providing a value which is expected to have error from both measurement and unexplained variance due to the simplifications in the scientific model.  Consider the typical formulation of a linear regression, where $y_n$ is the scalar outcome, $x_n$ is a row vector of predictors, $\beta$ is a coefficient vector parameter, and $\epsilon_n$ is a latent scalar parameter and $\sigma > 0$ is another parameter,
$$
\begin{eqnarray}
y_n & = & x_n \beta + \epsilon_n
\\[6pt]
\epsilon_n & \sim & \mbox{Normal}(0, \sigma)
\end{eqnarray}
$$
The deterministic part of the equation is the linear predictor $x \beta$.  The stochastic error term, $\epsilon_n$, gets a normal distribution located at zero with scale parameter $\sigma > 0$ (this error model ensures that the maximum likelihood value for $\beta$ is at the least squares solution).  We can alternatively write this model without the latent value $\epsilon_n$ as
$$
y_n \sim \mbox{Normal}(x_n \beta, \sigma).
$$
Here, $\epsilon_n = y_n - x_n \beta$ is implicit.

#### Noise model for Lotka-Volterra dynamics

The data $y_i$ consists of measurements of the prey $y_{i, 1}$ and predator $y_{i, 2}$ populations at times $t_i$.  The Lotka-Volterra equations will replace the determinsitic parts of the linear regression equations.

The true population sizes at time $t = 0$ are unknown---we only have measurements $y0_1$ and $y0_2$ for it.  The true population initial population sizes at time $t = 0$ will be represented by a parameter $z0$, so that

$$
z0_1 = u(t = 0)
\ \ \ \mathrm{and} \ \ \
z0_2 = v(t = 0).
$$

Next, let $z_1, \ldots, z_N$ be the solutions to the Lotka-Volterra differential equations at times $t_1, \ldots, t_N$ given initial conditions $z(t = 0) = z0$.  Each $z_n$ is a pair of prey and predator population sizes at the specified times,
$$
z_{n, 1} = u(t_n)
\ \ \ \mathrm{and} \ \ \
z_{n, 2} = v(t_n)
$$
The $z_n$ are deterministic functions of $z0$ and the system parameters $\alpha, \beta, \gamma, \delta$;  thus $z$ is not a parameter but a derived quantity.

The observed data is the form of measurements $y0$ of the initial population of prey and predators, and subsequent measurements $y_n$ at times $t_n$, where $y0$ and the $y_n$ consist of a pair of measured population sizes, for the prey and predator species.

Putting this together, the $y_n$ (and $y0$) are measurements of the underlying predicted population $z_n$ ($z0$).  Because they are positive, the noise will be modeled on the log scale.  This has the convenient side effect of making the error multiplicative (i.e., proportional) rather than additive.
$$
\begin{eqnarray}
\log y_{n, k} & = & \log z_{n, k} + \epsilon_{n, k}
\\[6pt]
\epsilon_{n, k} & \sim & \mathrm{Normal}(0, \sigma_k)
\end{eqnarray}
$$
where the $z_n$ are the solutions to the Lotka-Volterra equations at times $t_1, \ldots, t_N$ given initial population $z0$.  The prey and predator populations have error scales (on the log scale) of $\sigma_1$ and $\sigma_2$.


## Weakly informative priors

The only remaining question is what to use for priors on the parameters.   In general, the Stan Development Team has been recommending at least weakly informative priors.  In practice, the parameter ranges for the Lotka-Volterra model leading to stable populations are well known.

For the parameters,

$$
\begin{eqnarray}
\alpha, \gamma & \sim & \mathrm{Normal}(1, 0.5)
\\[6pt]
\beta, \delta & \sim & \mathrm{Normal}(0.05, 0.05)
\end{eqnarray}
$$

The noise scale is proportional, so the following prior should be weakly informative,
$$
\sigma \sim \mathrm{Lognormal}(0, 0.5)
$$

Then, for the initial population of predator and prey, the following priors are weakly informative
$$
\begin{eqnarray}
z_{0,1} & \sim & \mathrm{Normal}(\log(30), 1)
\\[6pt]
z_{0, 2} & \sim & \mathrm{Normal}(\log(5), 1)
\end{eqnarray}
$$

## Coding the model in Stan

#### Coding the system dynamics

Whenever a system of differential equations is involved, the system equations must be coded as a Stan function.  In this case, the model is relatively simple as the state is only two dimensional and there are only four parameters.  Stan requires the system to be defined with exactly the signature defined here for the function <code>dz_dt()</code>.  The first argument is for time, which is not used here because the Lotka-Voltarra equations are not time-dependent.  The second argument is for the system state, and here it is coded as an array
$z = (u, v)$.  The third argument is for the parameters of the equation, of which the Lotka-Voltarra equations have four, which are coded as $\theta = (\alpha, \beta, \gamma, \delta)$.  The fourth and fifth argument are for data constants, but none areneeded here, so these arguments are unused.

```
  real[] dz_dt(real t,       // time (unused)
               real[] z,     // system state
               real[] theta, // parameters
               real[] x_r,   // data (unused)
               int[] x_i) {
    real u = z[1];
    real v = z[2];

    real alpha = theta[1];
    real beta = theta[2];
    real gamma = theta[3];
    real delta = theta[4];

    real du_dt = (alpha - beta * v) * u;
    real dv_dt = (-gamma + delta * u) * v;

    return { du_dt, dv_dt };
  }
```
After unpacking the variables from their containers, the derivatives of population with respect to time are defined just as in the mathematical specification.  The return value uses braces to construct the two-element array to return, which consists of the derivatives of the system components with respect to time,
$$
\frac{\mathrm{d}}{\mathrm{d}t} z
\ = \ \frac{\mathrm{d}}{\mathrm{d}t} (u, v)
\ = \ \left( \frac{\mathrm{d}}{\mathrm{d}t} u, \, \frac{\mathrm{d}}{\mathrm{d}t} v \right).
$$

The data and parameters are coded following their specifications.

```
data {
  int<lower = 0> N;         // num measurements
  real ts[N];               // measurement times > 0
  real y0[2];               // initial measured population
  real<lower = 0> y[N, 2];  // measured population at measurement times
}
parameters {
  real<lower = 0> theta[4];  // theta = { alpha, beta, gamma, delta }
  real<lower = 0> z0[2];     // initial population
  real<lower = 0> sigma[2];  // measurement errors
}
```

The solutions to the Lotka-Volterra equations for a given initial state $z0$ are coded up as transformed parameters.  This will allow them to be used in the model and inspected in the output.  It also makes it clear that they are all functions of the initial population and parameters (as well as the solution times).
```
transformed parameters {
  // population for remaining years
  real z[N, 2]
    = integrate_ode_rk45(dz_dt, z0, 0, ts, theta,
                         rep_array(0.0, 0), rep_array(0, 0),
                         1e-6, 1e-5, 1e3);
}
```
The Runge-Kutta 4th/5th-order solver is specified here for efficiency (with suffix <code>_rk45</code>) because the equations are not stiff in the parameter ranges encountered for this data.   The required real and integer data arguments in the second line are both given as size-zero arrays.  The last line provides relative and absolute tolerances, along with the maximum number of steps allowed in the solver.  For further efficiency, the tolerances for the differential equation solver are relatively loose for this example; usually tighter tolerances are required (smaller numbers).

If the solver runs into stiffness (the symptom of which is very slow iterations that may appear to be hanging), it is best to switch to the backward-differentiation formula solver, called with <code>integrate_ode_bdf</code>. The Runge-Kutta solver is twice as fast as the BDF solver for this problem on this data.

With the solutions in hand, the only thing left are the prior and likelihood.  As with the other parts of the model, these directly follow the notation in the mathematical specification of the model.

```
model {
  // priors
  sigma ~ normal(0, 0.5);
  theta[1:2] ~ normal(0, 1);
  theta[3:4] ~ normal(0, 0.2);
  z0[1] ~ normal(10, 10);
  z0[2] ~ normal(50, 50);

  // likelihood
  y0 ~ lognormal(log(z0), sigma);
  for (k in 1:2)
    y[ , k] ~ lognormal(log(z[, k]), sigma[k]);
}
```

## Fitting the Hudson Bay Company lynx-hare data

First, the data is setup in a form suitable for Stan.
```{r}
N <- length(lynx_hare_df$Year) - 1                   # num observations after first
ts <- 1:N                                            # observation times just years
y0 <- c(lynx_hare_df$Hare[1], lynx_hare_df$Lynx[1])  # first observation
y <- as.matrix(lynx_hare_df[2:(N + 1), 2:3])         # remaining observations
y <- cbind(y[ , 2], y[ , 1]);                        # reverse order
lynx_hare_data <- list(N, ts, y0, y)
```

Next, the model is translated to C++ and compiled.

```{r results="hide"}
model <- stan_model("lotka-volterra.stan")
```

Finally, the compiled model and data are used for sampling.  Stan's default settings are sufficient for this data set and model.

```{r results="hide"}
fit <- sampling(model, data = lynx_hare_data,
                seed=123)
```

The output can be displayed in tabular form, here limited to the median (0.5 quantile) and 80% interval (0.1 and 0.9 quantiles).

```{r}
print(fit, probs=c(0.1, 0.5, 0.9), digits=3)
```

The R-hat values are all near 1, which is consistent with convergence.  The effective sample size estimates for each parameter are sufficient for inference.  Thus we have reason to trust this fit.

The expected values `z` are unlike the replicated draws `y_rep` in two ways.  First, their posterior has much lower variance and much narrower 80% intervals.  This is to be expected, as the `y_rep` additional takes into account measurement and unexplained variance, whereas `z` only takes into account parameter estimation uncertainty.  Second, the mean values of `z` are lower than the corresponding values of `y_rep`.  This is because `y_rep` is adding a lognormal error term, which has a positive expectation as it is constrained to be positive;  this positivity is also a factor in the fits that are derived for `z`.


#### Comparing the fitted model to data

Using a non-statistically motivated error term and optimization, Howard (2009, Figure 2.10) provides the following approximate point estimates for the model parameters based on the data.
$$
\hat{\alpha} = 0.55, \ \
\hat{\beta} = 0.028, \ \
\hat{\gamma} = 0.84, \ \
\hat{\delta} = 0.026
$$
Our model produced the following point estimates based on the posterior mean, which minimizes expected squared error,
$$
\hat{\alpha} = 0.55, \ \
\hat{\beta} = 0.028, \ \
\hat{\gamma} = 0.80, \ \
\hat{\delta} = 0.024
$$
and the posterior median, which minimizes expected absolute error,
$$
\hat{\alpha} = 0.54, \ \
\hat{\beta} = 0.035, \ \
\hat{\gamma} = 0.80, \ \
\hat{\delta} = 0.030.
$$
The estimates are very similar to each other and to Howard's.

Howard then plugs in these point estimates and derives the most likely populations $z$ (including the initial population $z0$).  Rather than plugging in point estimates to get point predictions, we will adjust for the two forms of uncertainty inherent in our model.  First, there is estimation uncertainty, which we characterize with the posterior density $p(\alpha, \beta, \gamma, \delta, z_0, \sigma \mid y)$.  The second form of uncertainty is the observation error and unexplained variation, which are both rolled into a single sampling distribution, $\log y_n \sim \mathsf{Normal}(\log z_n, \sigma)$.  As in the Stan implementation, $z_n$ is the solution to the differential equation conditioned on the parameters $\alpha, \beta, \gamma, \delta$ and initial state $z_0$.  Altogether, we will be repulating new $y$ values, which we write as $y^{\mathrm{rep}}$, according to the posterior predictive distribution,

$$
p(y^{\mathrm{rep}} | y)
\ = \
\int p(y^{\mathrm{rep}} | \theta) \ p(\theta | y) \ \mathrm{d}\theta.
$$
where $\theta = (\alpha, \beta, \gamma, \delta, z_0, \sigma)$ is the vector of parameters for the model.  Then, we calculate the posterior mean, which is itself an expectation,
$$
\begin{eqnarray}
\hat{y}^{\mathrm{rep}}
& = &
\mathbb{E}[y^{\mathrm{rep}} | y]
\\[8pt]
& = &
\int y^{\mathrm{rep}} \, p(y^{\mathrm{rep}} | y) \ \mathrm{d}y^{\mathrm{rep}}
\\[8pt]
& = &
\int y^{\mathrm{rep}} \, p(y^{\mathrm{rep}} | \theta) \, p(\theta | y) \ \mathrm{d}y^{\mathrm{rep}} \ \mathrm{d}\theta
\\[8pt]
& \approx &
\frac{1}{M} \sum_{m=1}^M y^{\mathrm{rep}(m)}
\end{eqnarray}
$$
As with other posterior expectations, the Bayesian point estimate is given by a simple average over simulated values, where $y^{\mathrm{rep}(m)}$ is just the result of simulating the value of $y^{\mathrm{rep}}$ according to the generative model based on parameter draw $\theta^{(m)}$.

The posterior predictive estimates of the dynamics are shown below, along with the raw data on number of pelts collected.

```{r}
z0_draws <- extract(fit)$z0
z_draws <- extract(fit)$z
y0_rep_draws <- extract(fit)$y0_rep
y_rep_draws <- extract(fit)$y_rep
predicted_pelts <- matrix(NA, 21, 2)
min_pelts <- matrix(NA, 21, 2)
max_pelts <- matrix(NA, 21, 2)
for (k in 1:2) {
  predicted_pelts[1, k] <- mean(y0_rep_draws[ , k])
  min_pelts[1, k] <- quantile(y0_rep_draws[ , k], 0.25)
  max_pelts[1, k] <- quantile(y0_rep_draws[ , k], 0.75)
  for (n in 2:21) {
    predicted_pelts[n, k] <- mean(y_rep_draws[ , n - 1, k])
    min_pelts[n, k] <- quantile(y_rep_draws[ , n - 1, k], 0.25)
    max_pelts[n, k] <- quantile(y_rep_draws[ , n - 1, k], 0.75)
  }
}

lynx_hare_melted_df <- melt(as.matrix(lynx_hare_df[, 2:3]))
colnames(lynx_hare_melted_df) <- c("year", "species", "pelts")
lynx_hare_melted_df$year <-
  lynx_hare_melted_df$year +
  rep(1899, length(lynx_hare_melted_df$year))

Nmelt <- dim(lynx_hare_melted_df)[1]
lynx_hare_observe_df <- lynx_hare_melted_df
lynx_hare_observe_df$source <- rep("measurement", Nmelt)

lynx_hare_predict_df <-
  data.frame(year = rep(1900:1920, 2),
             species = c(rep("Lynx", 21), rep("Hare", 21)),
             pelts = c(predicted_pelts[, 2],
                       predicted_pelts[, 1]),
             min_pelts = c(min_pelts[, 2], min_pelts[, 1]),
             max_pelts = c(max_pelts[, 2], max_pelts[, 1]),
             source = rep("prediction", 42))

lynx_hare_observe_df$min_pelts = lynx_hare_predict_df$min_pelts
lynx_hare_observe_df$max_pelts = lynx_hare_predict_df$max_pelts

lynx_hare_observe_predict_df <-
  rbind(lynx_hare_observe_df, lynx_hare_predict_df)

population_plot2 <-
  ggplot(data = lynx_hare_observe_predict_df,
         aes(x = year, y = pelts, color = source)) +
  facet_wrap( ~ species, ncol = 1) +
  geom_ribbon(aes(ymin = min_pelts, ymax = max_pelts),
	      colour = NA, fill = "black", alpha = 0.1) +
  geom_line() +
  geom_point() +
  ylab("pelts (thousands)") +
  ggtitle("Posterior predictive replications with 50% intervals\nvs. measured data")
population_plot2
```

This posterior predictive check shows that the model fit is consistent with the data, with around 50% of the data points falling within the 50% intervals.


## How large are the populations?

Going on the assumption that the number of pelts collected is proportional to the population, we only know how the relative sizes of the populations change, not their actual sizes.

This model could be combined with a mark-recapture model to get a better handle on the actual population size.  Mark-recapture gives you an estimate of actual numbers and the Lotka-Volterra model would provide information on relative change in the predator and prey populations.

## Extensions to the model

The Lotka-Volterra model is easily extended for realistic applications in several ways.

1.  Predictors can be rolled into the system state to take into the dynamnics to account for things like the correlation of populations with the abundance of food.

2.  The model may be extended beyond two species.  The dynamics for each species will reflect that it may stand in predator-prey relations to multiple other species.

3.  Additional data for population observations may be included, such as adding a mark-recapture model for tag-release-recapture data of populations.


## Exercises

1.  Extend predictions another 50 years into the future and plot as in the last plot.  This can be done by extending the solution points in the transformed parameters, but is more efficiently done in the generated quantities block.

2.  Write a Stan model to simulate data from this model.  First simulate parameters from the prior (or pick ones consistent with the priors).  Then simulate data from the parameters.  Finally, fit the model in Stan and compare the coverage as in the last plot in the case study.

3.  Suppose that several of the measurements are missing.  Write a Stan program that uses only the observed measurements.  How robust is the fit to missing a few data points?

4.  Write a Stan model that predicts the population at finer-grained intervals than a year (such as every three months).  Can the model be formulated to only use the yearly data?  Do the smoother plots for predicted populations make sense?  Does this fit better or worse than the original model?

4.  Replace the lognormal error with a simple normal error model.  What does this do to the `z` estimates and to the basic parameter estimates?  Which error model fits better?


## References

* Howard, P. (2009). Modeling Basics. Lecture Notes for Math 442, Texas A&M University.

* Lotka, A. J. (1925). *Principles of physical biology*. Baltimore: Waverly.

* Volterra, V. (1926). Fluctuations in the abundance of a species considered mathematically. *Nature*, 118(2972), 558-560.

* Volterra, V. (1927). *Variazioni e fluttuazioni del numero d'individui in specie animali conviventi*. C. Ferrari.

<br />

### Appendix: Session information

```{r}
sessionInfo()
```

<br />

### Appendix: Licenses

* Code &copy; 2017, Columbia University, licensed under BSD-3.
* Text &copy; 2017, Bob Carpenter, licensed under CC-BY-NC 4.0.
