---
title: "Two-parameter logistic model with latent regression"
author: "Daniel C. Furr"
date: "`r gsub('(^.* )(0)([0-9], .*)', '\\1\\3', format(Sys.time(), '%B %d, %Y'))`"
output:
  html_document:
    toc: true
    number_sections: true
    fig_caption: true
    css: ../styles.css
bibliography: ../bibliography.bib
---

<!-- 
(Title:)  Two-parameter logistic model with latent regression

Author:  Daniel C. Furr

Date:  2016

Abstract:  This case study documents a Stan model for the two-parameter logistic model (2PL) with latent regression. The latent regression portion of the model may be restricted to an intercept only, yielding a standard 2PL. A brief simulation indicates that the Stan model successfully recovers the generating parameters. An example using a grade 12 science assessment is provided.

Keywords:  education, IRT, 2PL. 
--> 

```{r knitr_opts, include = FALSE}
knitr::opts_chunk$set(tidy = TRUE, cache = TRUE)
```


# Model

## Overview

The two-parameter logistic model (2PL) [@swaminathan1985bayesian] is an item response theory model that includes parameters for both the difficulty and discrimination of dichotomous items. 
The version presented includes a latent regression. However, the latent regression part of the model may be restricted to an intercept only, resulting in a regular 2PL.

$$
  \mathrm{logit} [ \Pr(y_{ij} = 1 | \alpha_i, \beta_i, \theta_j, \lambda) ] =
  \alpha_i (\theta_j + w_j' \lambda - \beta_i)
$$
$$
  \theta_j \sim \mathrm{N}(0, 1)
$$

Variables:

* $i = 1 \ldots I$ indexes items.
* $j = 1 \ldots J$ indexes persons.
* $y_{ij} \in \{ 0 , 1 \}$ is the response of person $j$ to item $i$.
* $w_{j}$ is the vector of covariates for person $j$, which will in general include an intercept term.

Parameters:

* $\alpha_i$ is the discrimination for item $i$.
* $\beta_i$ is the difficulty for item $i$.
* $\theta_j$ is the ability for person $j$.
* $\lambda$ is the vector of latent regression parameters.

Constraints:

* The last item difficulty is constrained to be the negative sum of the other difficulties, $\beta_I = -\sum_{i}^{(I-1)} \beta_i$, resulting in the average item difficulty being zero.

Priors:

* $\alpha_i \sim \mathrm{log~N}(1, 1)$ is weakly informative, except that positive discrimination parameters are assumed.
* $\beta_1 \ldots \beta_{I-1} \sim \mathrm{N}(0, 5)$ is weakly informative, and no prior is needed for the contrained difficulty $\beta_I$.
* A uniform prior is chosen for $\lambda$ because the scale of the person covariates will vary depending on the application.


## **Stan** program

The **Stan** program is presented below. The constraint is placed on the last item difficulty by creating `beta_free`, which is the vector of unconstrained item parameters, in the parameters block. Then in the transformed parameters block, `beta` is created to be identical to `beta_free` except for one additional element that is the constrained item difficulty.

No priors are specified for `lambda`, the regression coefficients, because the scale of the person covariates may very greatly depending on the application. In the absence of a specified prior, **Stan** will assign the uniform prior. Readers should consider adding appropriate priors for `lambda` to the **Stan** program according to their knowledge of the problem.

```{r stan_code, comment="", echo=FALSE}
cat(readLines("2pl_latent_reg.stan"), sep = "\n")
```


# Simulation

The **Stan** model is fit to a simulated dataset to evaluate it's ability to recover the generating parameter values. The simulation is conducted in **R** and analysis proceeds using the **rstan** implementation of **Stan**.

First, the necessary **R** packages are loaded.

```{r, message=FALSE, warning=FALSE, results="hide"}
# Load R packages
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(ggplot2)
```

The **R** code that follows simulates a dataset conforming to the model.

```{r sim_data}
# Set paramters for the simulated data
I <- 20
J <- 500
sigma <- 1
lambda <- c(.5, .5, .5)
W <- cbind(1, rnorm(J, 0, 1), rnorm(J, 0, 1))
alpha <- rep(c(.8, 1, 1.2, 1.4),  length.out = I)
beta_free <- seq(from = -2, to = 2, length.out = I-1)

# Calculate or sample remaining paramters
theta <-  rnorm(J, 0, sigma)
mu <-  W %*% matrix(lambda)
beta <- c(beta_free, -1 * sum(beta_free))

# Assemble data and simulate response
sim_data <- list(I = I,
                 J = J,
                 N = I*J,
                 ii = rep(1:I, times = J),
                 jj = rep(1:J, each = I),
                 K = ncol(W),
                 W = W)
eta <- alpha[sim_data$ii]*theta[sim_data$jj] + mu[sim_data$jj] - beta[sim_data$ii]
sim_data$y <- as.numeric(boot::inv.logit(eta) > runif(sim_data$N))
```

The simulated data consists of `r I` dichotomous items and `r J` persons. The latent regression includes an intercept and `r length(lambda) - 1` person-related covariates, which are standard normal variables. The simulated dataset is next fit with **Stan**.

```{r sim_fit, result="hide"}
# Fit model to simulated data
sim_fit <- stan(file = "2pl_latent_reg.stan",
                data = sim_data, chains = 4, iter = 1000)
```

Before interpreting the results, it is necessary to check that the chains have converged. **Stan** provides the $\hat{R}$ statistic for the model parameters and log posterior. These are provided in the following figure. All values for $\hat{R}$ should be less than 1.1.

```{r sim_converge, fig.cap="Convergence statistics ($\\hat{R}$) by parameter for the simulation. All values should be less than 1.1 to infer convergence."}
# Plot of convergence statistics
sim_monitor <- as.data.frame(monitor(sim_fit, print = FALSE))
sim_monitor$Parameter <- as.factor(gsub("\\[.*]", "", rownames(sim_monitor)))
ggplot(sim_monitor) +
  aes(x = Parameter, y = Rhat, color = Parameter) +
  geom_jitter(height = 0, width = .5, show.legend = FALSE) +
  ylab(expression(hat(italic(R))))
```

The **Stan** model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% posterior intervals for the difference. Ideally, (nearly) all the 95% posterior intervals would include zero.

```{r sim_plot, fig.height=8, fig.cap="Discrepancies between estimated and generating parameters for the simulation. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters."}
# Make vector of wanted parameter names
wanted_pars <- c(paste0("alpha[", 1:sim_data$I, "]"),
                 paste0("beta[", 1:sim_data$I, "]"),
                 paste0("lambda[", 1:sim_data$K, "]"))

# Get estimated and generating values for wanted parameters
generating_values = c(alpha, beta, lambda)
estimated_values <- sim_monitor[wanted_pars, c("mean", "2.5%", "97.5%")]

# Assesmble a data frame to pass to ggplot()
sim_df <- data.frame(parameter = factor(wanted_pars, rev(wanted_pars)),
                     row.names = NULL)
sim_df$middle <- estimated_values[,"mean"] - generating_values
sim_df$lower <- estimated_values[,"2.5%"] - generating_values
sim_df$upper <- estimated_values[,"97.5%"] - generating_values

# Plot the discrepancy
ggplot(sim_df) +
  aes(x = parameter, y = middle, ymin = lower, ymax = upper) +
  scale_x_discrete() +
  labs(y = "Discrepancy", x = NULL) +
  geom_abline(intercept = 0, slope = 0, color = "white") +
  geom_linerange() +
  geom_point(size = 2) +
  theme(panel.grid = element_blank()) +
  coord_flip()
```


# Example application

The example data are from the The First International Mathematics Study [@husen1967international; @postlethwaite1967school]. The data include information about student gender and country (Australia or Japan). For convenience, only a subset of the full data are used. 

```{r example_data}
# Attach the example dataset. The TAM package is required.
data(data.fims.Aus.Jpn.scored, package = "TAM")

# Subset the full data
select <- floor(seq(from = 1, to = nrow(data.fims.Aus.Jpn.scored),
                    length.out = 500))
subsetted_df <- data.fims.Aus.Jpn.scored[select, ]
str(subsetted_df)
```

The dataset is next divided into an item response matrix and a matrix of student covariates.

```{r example_respmatrix}
# Extract the response matrix
response_matrix <- as.matrix(subsetted_df[, grepl("^M1", names(subsetted_df))])
head(response_matrix)
```

```{r example_covariates}
# Assemble a matrix of person covariates
male <- as.numeric(subsetted_df$SEX == 2)
japan <- as.numeric(subsetted_df$country == 2)
W = cbind(intercept = 1,
          male = male,
          japan = japan,
          interact = male*japan)
head(W)
```

`r nrow(W)` students responded to `r ncol(W)` dichotomously scored items. The data contain no missing values. The two matrices are converted to a list suitable for the **Stan** model.

```{r example_fit, , result="hide"}
# Assemble data list and fit model
ex_list <- list(I = ncol(response_matrix),
                J = nrow(response_matrix),
                N = length(response_matrix),
                ii = rep(1:ncol(response_matrix), each = nrow(response_matrix)),
                jj = rep(1:nrow(response_matrix), times = ncol(response_matrix)),
                y = as.vector(response_matrix),
                K = ncol(W),
                W = W)
ex_fit <- stan(file = "2pl_latent_reg.stan", 
                data = ex_list, chains = 4, iter = 1000)
```

As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior density, using $\hat{R}$.

```{r example_converge, fig.cap="Convergence statistics ($\\hat{R}$) by parameter for the example. All values should be less than 1.1 to infer convergence."}
# Plot of convergence statistics
ex_monitor <- as.data.frame(monitor(ex_fit, print = FALSE))
ex_monitor$Parameter <- as.factor(gsub("\\[.*]", "", rownames(ex_monitor)))
ggplot(ex_monitor) +
  aes(x = Parameter, y = Rhat, color = Parameter) +
  geom_jitter(height = 0, width = .5, show.legend = FALSE) +
  ylab(expression(hat(italic(R))))
```

Next we view summaries of the parameter posteriors.

```{r example_print}
# View table of parameter posteriors
print(ex_fit, pars = c("alpha", "beta", "lambda"))
```

A 2PL without the latent regression could be fit by changing the person covariate matrix to include only an intercept term. Shown below is how this may be done for the example data.

```{r example_noreg, eval = FALSE}
# Assemble data list and fit model
noreg_list <- ex_list
noreg_list$W <- matrix(1, nrow = ex_list$J, ncol = 1)
noreg_list$K <- 1
noreg_fit <- stan(file = "2pl_latent_reg.stan", 
                  data = noreg_list, chains = 4, iter = 1000)
```


# References

<!-- This comment causes section to be numbered -->
