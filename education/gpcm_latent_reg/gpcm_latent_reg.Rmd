---
title: "Generalized partial credit model with latent regression"
author: "Daniel C. Furr"
date: "`r gsub('(^.* )(0)([0-9], .*)', '\\1\\3', format(Sys.time(), '%B %d, %Y'))`"
output:
  html_document:
    toc: true
    number_sections: true
    fig_caption: true
    css: ../styles.css
bibliography: ../bibliography.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(tidy = TRUE, cache = TRUE)
```


# Model

## Overview

The generalized partial credit model [@muraki1992generalized] is appropriate for item response data that features more than two *ordered* response categories. The items may have differing numbers of response categories. For dichotomous items (items with exactly two response categories), the generalized partical credit model is equivalent to the two-parameter logistic model. The version presented includes a latent regression. However, the latent regression may be restricted to a model intercept, resulting in the standard generalized partial credit model. Also, we use the item-intercept parameterization of the model and separate the latent regression from the product of the ability and discrimination parameters for better performance.

$$
\mathrm{logit} [ \Pr(Y_{ij} = y,~y > 0 | \theta_j, \alpha_i, \beta_i) ] =
\frac{\exp \sum_{s=1}^y \alpha_i  (\theta_j + w_{j}' \lambda - \beta_{is})}
     {1 + \sum_{k=1}^{m_i} \exp \sum_{s=1}^k 
       \alpha_i (\theta_j + w_{j}' \lambda - \beta_{is})}
$$
$$
\mathrm{logit} [ \Pr(Y_{ij} = y,~y = 0 | \theta_j, \alpha_i, \beta_i) ] =
\frac{1}
     {1 + \sum_{k=1}^{m_i} \exp \sum_{s=1}^k 
       \alpha_i (\theta_j + w_{j}' \lambda - \beta_{is})}
$$
$$
\theta_j \sim \mathrm{N}(0, 1)
$$

Variables:

* $i = 1 \ldots I$ indexes items.
* $j = 1 \ldots J$ indexes persons.
* $y_{ij} \in \{ 0 \ldots m_i \}$ is the response of person $j$ to item $i$
* $m_i$ is simulataneously the maximum score and number of step difficulty parameters for item $i$.
* $w_{j}$ is the vector of covariates for person $j$, which will in general include an intercept term.

Parameters:

* $\alpha_i$ is the discrimination for item $i$.
* $\beta_{is}$ is the $s$-th step difficulty for item $i$.
* $\theta_j$ is the ability for person $j$.
* $\lambda$ is the vector of latent regression parameters.

Constraints:

* The last difficulty parameter is constrained to be the negative sum of the other difficulties, resulting in the average difficulty parameter being zero.

Priors:

* $\alpha_i \sim \mathrm{log~N}(1, 1)$ is weakly informative, except that positive discrimination parameters are assumed.
* $\beta_1 \ldots \beta_{I-1} \sim \mathrm{N}(0, 5)$ is weakly informative, and no prior is needed for the contrained difficulty $\beta_I$.
* A uniform prior is chosen for $\lambda$ because the scale of the person covariates will vary depending on the application.


## **Stan** program

A few aspects of the **Stan** program for the generalized partial credit model bear mentioning. First, the prediction for person ability is calculated and temporarily stored in `mu` in the model block. This is done for efficiency and readability of the code.

Second, the program begins with the creation of a user-specified function `gpcm_probs()`, which accepts values for `theta`, `mu`, and `alpha` and a vector of parameters `beta` for one item. With these inputs, it returns a vector of model-predicted probabilities for each possible response. Later, in the model block, `gpcm_probs()` is used to get the likelihood of the observed item responses.

Third, the encoding of item responses are modified such that the lowest response category is one instead of zero. This modification takes place in the transformed data block, in which a new variable `r` is created for this purpose. The adjustment is necessary for compatibility with the `categorical()` function.

Fourth, the variables `m` and `pos` are also created in the transformed data block. These are needed to pick out the vector of item parameters for a single item from the vector of all item parameters `beta`. `pos` indicates the position of the first parameter for a given item, while `m` indicates the count of parameters for an item. 

Lastly, a constraint is placed on the item difficulties for model identification. This is accomplished by creating `beta_free`, which is the vector of unconstrained item parameters, in the parameters block. Then in the transformed parameters block, `beta` is created to be identical to `beta_free` except for one additional element that is the constrained item difficulty. As a result of this constraint, the mean of `beta` will be zero.


```{r stan_code, comment="", echo=FALSE}
cat(readLines("gpcm_latent_reg.stan"), sep = "\n")
```


# Simulation

```{r, message=FALSE, warning=FALSE, results="hide"}
# Load R packages
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(ggplot2)
```

The **R** code that follows simulates a dataset conforming to the model. The **Stan** model will be evaluated in terms of its ability to recover the generating values of the parameters when fit to this dataset.

```{r sim_data}
# Person covariates and abilities
J <- 500
lambda <- c(.5, .5, .5)
W <- cbind(1, rnorm(J, 0, 1), rnorm(J, 0, 1))
mu <- W %*% matrix(lambda)
theta <- rnorm(J, 0, 1)

# Item parameters
I <- 20
alpha <- rep(c(.8, 1, 1.2), length.out = I)
Beta_uncentered <- matrix(NA, nrow = I, ncol = 2)
Beta_uncentered[,1] <- seq(from = -1.5, to = 1.5, length.out = I)
Beta_uncentered[,2] <- Beta_uncentered[,1] + rep(c(.25, .5, .75, 1), length.out = I)
Beta_centered <- Beta_uncentered - mean(Beta_uncentered)
beta <- as.vector(t(Beta_centered))

# Start of Stan data list
data_list <- list(I = I,
                  J = J,
                  N = I*J,
                  ii = rep(1:I, times = J),
                  jj = rep(1:J, each = I),
                  K = ncol(W), 
                  W = W)

# Function to simulate responses
simulate_response <- function(theta, mu, alpha, beta) {
  unsummed <- c(0, alpha*(theta + mu - beta))
  numerators <- exp(cumsum(unsummed))
  denominator <- sum(numerators)
  response_probs <- numerators/denominator
  simulated_y <- sample(1:length(response_probs) - 1, size = 1, 
                        prob = response_probs) 
  return(simulated_y)
}

# Add simulated responses to Stan data list
data_list$y <- numeric(data_list$N)
for(n in 1:data_list$N) {
  data_list$y[n] <- simulate_response(theta[data_list$jj[n]],
                                      mu[data_list$jj[n]],
                                      alpha[data_list$ii[n]],
                                      Beta_centered[data_list$ii[n], ])
}
```

The simulated data consists of `r I` items, each with `r ncol(Beta_uncentered) + 1` categories, and `r J` persons. The latent regression includes an intercept and `r length(lambda) - 1` person-related covariates, which are standard normal variables. Next, the model is fit to the simulated dataset with **Stan**.

```{r sim_fit, result="hide"}
# Fit model to simulated data
sim_fit <- stan(file = "gpcm_latent_reg.stan", 
                data = data_list, chains = 4, iter = 500)
```

Before interpreting the results, it is necessary to check that the chains have converged. **Stan** provides the $\hat{R}$ statistic for the model parameters and log posterior. These are provided in the following figure. All values for $\hat{R}$ should be less than 1.1.

```{r sim_converge, fig.cap="Convergence statistics ($\\hat{R}$) by parameter for the simulation. All values should be less than 1.1 to infer convergence."}
# Plot of convergence statistics
sim_summary <- as.data.frame(summary(sim_fit)[[1]])
sim_summary$Parameter <- as.factor(gsub("\\[.*]", "", rownames(sim_summary)))
ggplot(sim_summary) +
  aes(x = Parameter, y = Rhat, color = Parameter) +
  geom_jitter(height = 0, width = .5, show.legend = FALSE) +
  ylab(expression(hat(italic(R))))
```

The **Stan** model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% posterior intervals for the difference. Ideally, (nearly) all the 95% posterior intervals would include zero.

```{r sim_plot, fig.height=8, fig.cap="Discrepancies between estimated and generating parameters for the simulation. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters."}
# Make vector of wanted parameter names
wanted_pars <- c(paste0("alpha[", 1:length(alpha), "]"), 
                 paste0("beta[", 1:length(beta), "]"), 
                 paste0("lambda[", 1:ncol(W), "]"))

# Get estimated and generating values for wanted parameters
generating_values = c(alpha, beta, lambda)
estimated_values <- sim_summary[wanted_pars, c("mean", "2.5%", "97.5%")]

# Assesmble a data frame to pass to ggplot()
sim_df <- data.frame(parameter = factor(wanted_pars, rev(wanted_pars)),
                     row.names = NULL)
sim_df$middle <- estimated_values[,"mean"] - generating_values
sim_df$lower <- estimated_values[,"2.5%"] - generating_values
sim_df$upper <- estimated_values[,"97.5%"] - generating_values

# Plot the discrepancy
ggplot(sim_df) +
  aes(x = parameter, y = middle, ymin = lower, ymax = upper) +
  scale_x_discrete() +
  labs(y = "Discrepancy", x = NULL) +
  geom_abline(intercept = 0, slope = 0, color = "white") +
  geom_linerange() +
  geom_point(size = 2) +
  theme(panel.grid = element_blank()) +
  coord_flip()
```


# Example application

The example data are from the TIMSS 2011 mathematics assessment [@mullis2012timss] of Australian and Taiwanese students. For convenience, a subset of 500 students is used. The subsetted data is then divided into a person covariate matrix and an item response matrix.

```{r example_subset_data}
# Attach the example dataset. The TAM package is required.
data(data.timssAusTwn.scored, package = "TAM")

# Subset the full data
select <- floor(seq(from = 1, to = nrow(data.timssAusTwn.scored), 
                    length.out = 500))
subsetted_df <- data.timssAusTwn.scored[select, ]
str(subsetted_df)
```


The dataset is next divided into an item response matrix and a matrix of student covariates.

```{r example_separate_matrices}
# Make a matrix of person predictors
w_mat <- cbind(intercept = rep(1, times = nrow(subsetted_df)),
               taiwan = as.numeric(subsetted_df$IDCNTRY == 158),
               female = as.numeric(subsetted_df$ITSEX == 2),
               book14 = as.numeric(subsetted_df$IDBOOK == 14))
head(w_mat)

# Make a matrix of item responses
y_mat <- as.matrix(subsetted_df[, grep("^M", names(subsetted_df))])
head(y_mat)
```

The person covariate matrix `w_mat` has columns representing an intercept and three indicator variables for being in Taiwan (versus Australia), being female (versus male), and being assigned test booklet 14 (instead of booklet 1). The item response matrix `y_mat` contains `r ncol(y_mat)` items. Neither the response matrix or person covariates contain missing data.

Before fitting the model, some descriptive statistics are considered. First, a frequency table for the person covariates is created.

```{r example_descriptives_1}
# Customized version of table() that does not omit missing categories. Specify
# expected categories with key.
consec_table <- function(x, key) {
  y <- rep(0, times = length(key))
  names(y) <- as.character(key)
  x_table <- table(x)
  y[names(x_table)] <- x_table
  return(y)
}

# Frequency table for person covariates
t(apply(w_mat, 2, consec_table, key = 0:1))
```

Next, a frequency table for item responses is considered.

```{r example_descriptives_2}
# Frequency table for item responses
item_freqs <- t(apply(y_mat, 2, consec_table, key = 0:2))
item_freqs
```

The above table shows that the data are a mixture of dichotomous items and polytomous items having three responses categories. The first and second items are dichotomous, while the third and fourth are polytomous, for example. Consequently, the first and second items will have one step parameter each, while the third and fourth will have two each. *If* this table indicated there were no 0 or 1 responses for one of the polytomous items, responses for that item would need to be recoded.

Because all item parameters are stored in one vector, `beta`, some care is required in understanding which elements of `beta` correspond to which items. The following **R** code produces a table that maps each element in `beta` to its associated item and step.

```{r example_beta_key}
# Make a table mapping elements of beta to items/steps
x <- item_freqs[,-1] > 0
beta_key_trans <- t(matrix(NA, ncol = ncol(x), nrow = nrow(x)))
beta_key_trans[t(x[])] <- 1:sum(x)
beta_key <- t(beta_key_trans)
rownames(beta_key) <- rownames(item_freqs)
colnames(beta_key) <- paste("Step", 1:ncol(beta_key))
beta_key
```

The data are now formatted into a list and fit with **Stan**.

```{r example_fit, , result="hide"}
# Assemble data list for Stan
ex_list <- list(I = ncol(y_mat),
                J = nrow(y_mat),
                N = length(y_mat),
                ii = rep(1:ncol(y_mat), each = nrow(y_mat)),
                jj = rep(1:nrow(y_mat), times = ncol(y_mat)),
                y = as.vector(y_mat),
                K = ncol(w_mat),
                W = w_mat)

# Run Stan model
ex_fit <- stan(file = "gpcm_latent_reg.stan",
               data = ex_list, chains = 4, iter = 500)
```

As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior density, using $\hat{R}$.

```{r ex_converge, fig.cap="Convergence statistics ($\\hat{R}$) by parameter for the example. All values should be less than 1.1 to infer convergence."}
# Plot of convergence statistics
ex_summary <- as.data.frame(summary(ex_fit)[[1]])
ex_summary$Parameter <- as.factor(gsub("\\[.*]", "", rownames(ex_summary)))
ggplot(ex_summary) +
  aes(x = Parameter, y = Rhat, color = Parameter) +
  geom_jitter(height = 0, width = .5, show.legend = FALSE) +
  ylab(expression(hat(italic(R))))
```

Next we view summaries of the parameter posteriors.

```{r example_print}
# View table of parameter posteriors
print(ex_fit, pars = c("alpha", "beta", "lambda"))
```

If person covariates are unavailable, or their inclusion unwanted, the model may be fit restricting the matrix of person covariates to an intercept only. In this case, the vector `lambda` contains only one element, which will represent the mean of the ability distribution. The code below is an example of how to structure the data for **Stan** for this purpose.

```{r example_noreg, eval=FALSE}
# Fit the example data without latent regression
noreg_list <- ex_list
noreg_list$K <- 1
noreg_list$W <- matrix(1, nrow = nrow(M), ncol = 1)
noreg_fit <- stan(file = "gpcm_latent_reg.stan",
                data = noreg_list, chains = 4, iter = 500)
```


# References

<!-- This comment causes section to be numbered -->
