---
title: "Generalized rating scale model with latent regression"
author: "Daniel C. Furr"
date: "`r gsub('(^.* )(0)([0-9], .*)', '\\1\\3', format(Sys.time(), '%B %d, %Y'))`"
output:
  html_document:
    toc: true
    number_sections: true
    fig_caption: true
    css: ../styles.css
bibliography: ../bibliography.bib
---

<!-- 
(Title:)  Generalized rating scale model with latent regression

Author:  Daniel C. Furr

Date:  2016

Abstract:  This case study documents a Stan model for the generalized rating scale model (GRSM) with latent regression. The latent regression portion of the model may be restricted to an intercept only, yielding a standard GRSM A brief simulation indicates that the Stan model successfully recovers the generating parameters. An example using a survey of public perceptions of science and technology is provided.

Keywords:  education, IRT, GRSM. 
--> 

```{r, include = FALSE}
knitr::opts_chunk$set(tidy = TRUE, cache = TRUE)
```


# Model

## Overview

The generalized rating scale model is appropriate for item response data that involves Likert scale responses. The version presented includes a latent regression. However, the latent regression part of the model may be restricted to an intercept only, resulting in the standard partial credit model. Also, we use the item-intercept parameterization of the model and separate the latent regression from the product of the ability and discrimination parameters for better performance.

$$
\mathrm{logit} [ \Pr(Y_{ij} = y,~y > 0 | \theta_j, \lambda, \alpha_i, \beta_i, \kappa_s) ] =
\frac{\exp \sum_{s=1}^y (\alpha_i \theta_j + w_{j}' \lambda - \beta_i - \kappa_s)}
     {1 + \sum_{k=1}^{m} \exp \sum_{s=1}^k (\alpha_i \theta_j + w_{j}' \lambda - \beta_i - \kappa_s)}
$$
$$
\mathrm{logit} [ \Pr(Y_{ij} = y,~y = 0 | \theta_j, \lambda, \alpha_i, \beta_i, \kappa_s) ] =
\frac{1}
     {1 + \sum_{k=1}^{m} \exp \sum_{s=1}^k (\alpha_i \theta_j + w_{j}' \lambda - \beta_i - \kappa_s)}
$$
$$
\theta_j \sim \mathrm{N}(0, \sigma^2)
$$

Variables:

* $i = 1 \ldots I$ indexes items
* $j = 1 \ldots J$ indexes persons
* $y_{ij} \in \{ 0 \ldots m_i \}$ is the response of person $j$ to item $i$
* $m$ is simulataneously the maximum score and number of step difficulty parameters
* $w_{j}$ is the vector of covariates for person $j$

Parameters:

* $\alpha_i$ is the item-specific discrimination for item $i$
* $\beta_i$ is the item-specific difficulty for item $i$
* $\kappa_s$ is the $s$-th step difficulty, constant across items
* $\theta_j$ is the ability for person $j$
* $\lambda$ is the vector of latent regression parameters
* $\sigma^2$ is the variance for the ability distribution


## **Stan** program

A few aspects of the **Stan** program for the partial credit model bear mentioning. First, the prediction for person ability is calculated and temporarily stored in `mu` in the model block. This is done for efficiency and readability of the code.

Second, the program involves a user-specified function `grsm_probs()`, which accepts a value for `theta`, `mu` and `beta` as well as a vector for `kappa`. With these inputs, it returns a vector of model-predicted probabilities for each possible response. Later, in the model block, `grsm_probs()` is used to get the likelihood of the observed item responses.

Third, the encoding of item responses are modified such that the lowest response category is one instead of zero. This modification takes place in the transformed data block, in which a new variable `r` is created for this purpose. The adjustment is necessary for compatibility with the `categorical()` function.

Lastly, a constraint is placed on both `beta` and `kappa` for model identification. For `beta`, this is accomplished by creating `beta_free`, which is the vector of unconstrained item parameters, in the parameters block. Then in the transformed parameters block, `beta` is created to be identical to `beta_free` except for one additional element that is the constrained item difficulty. As a result of this constraint, the mean of `beta` will be zero. In a parallel way, `kappa` includes one constrained item parameter not found in `kappa_free`.

```{r stan_code, comment="", echo=FALSE}
cat(readLines("grsm_latent_reg.stan"), sep = "\n")
```


# Simulation

First, the necessary **R** packages are loaded.

```{r, message=FALSE, warning=FALSE, results="hide"}
# Load R packages
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
library(ggplot2)
```

The **R** code that follows simulates a dataset conforming to the model. The **Stan** model will be evaluated in terms of its ability to recover the generating values of the parameters when fit to this dataset.

```{r sim_data}
# Person covariates and abilities
J <- 500
lambda <- c(.5, .5, .5)
W <- cbind(1, rnorm(J, 0, 1), rnorm(J, 0, 1))
mu <- W %*% matrix(lambda)
theta <- rnorm(J, 0, 1)

# Item parameters
I <- 20
S <- 5
alpha <- rep(c(.8, 1, 1.2), length.out = I)
beta_uncentered <- seq(from = -1, to = 1, length.out = I)
beta <- beta_uncentered - mean(beta_uncentered)
kappa <- seq(from = -1, to = 1, length.out = S - 1)

# Start of Stan data list
sim_data <- list(I = I,
                 J = J,
                 N = I*J,
                 ii = rep(1:I, times = J),
                 jj = rep(1:J, each = I),
                 K <- ncol(W), 
                 W <- W)

# Function to simulate responses
simulate_response <- function(theta, mu, alpha, beta, kappa) {
  unsummed <- c(0, alpha*theta + mu - beta - kappa)
  numerators <- exp(cumsum(unsummed))
  denominator <- sum(numerators)
  response_probs <- numerators/denominator
  simulated_y <- sample(1:length(response_probs) - 1, size = 1,
                        prob = response_probs)
  return(simulated_y)
}

# Add simulated responses to Stan data list
sim_data$y <- numeric(sim_data$N)
for(n in 1:sim_data$N) {
  sim_data$y[n] <- simulate_response(theta[sim_data$jj[n]],
                                     mu[sim_data$jj[n]],
                                     alpha[sim_data$ii[n]],
                                     beta[sim_data$ii[n]],
                                     kappa)
}
```

The simulated data consists of `r I` items, each with `r S` categories, and `r J` persons. The latent regression includes an intercept and `r length(lambda) - 1` person-related covariates, which are standard normal variables. Next, the model is fit to the simulated dataset with **Stan**.

```{r sim_fit, results='hide'}
#Fit model to simulated data
sim_fit <- stan(file = "grsm_latent_reg.stan",
                data = sim_data, chains = 4, iter = 1000)
```

Before interpreting the results, it is necessary to check that the chains have converged. **Stan** provides the $\hat{R}$ statistic for the model parameters and log posterior. These are provided in the following figure. All values for $\hat{R}$ should be less than 1.1.

```{r sim_converge, fig.cap="Convergence statistics ($\\hat{R}$) by parameter for the simulation. All values should be less than 1.1 to infer convergence."}
# Plot of convergence statistics
sim_monitor <- as.data.frame(monitor(sim_fit, print = FALSE))
sim_monitor$Parameter <- as.factor(gsub("\\[.*]", "", rownames(sim_monitor)))
ggplot(sim_monitor) +
  aes(x = Parameter, y = Rhat, color = Parameter) +
  geom_jitter(height = 0, width = .5, show.legend = FALSE) +
  ylab(expression(hat(italic(R))))
```

The **Stan** model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% posterior intervals for the difference. Ideally, (nearly) all the 95% posterior intervals would include zero.

```{r sim_plot, fig.height=8, fig.cap="Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters."}
# Make vector of wanted parameter names
wanted_pars <- c(paste0("alpha[", 1:length(alpha), "]"),
                 paste0("beta[", 1:length(beta), "]"),
                 paste0("kappa[", 1:length(kappa), "]"),
                 paste0("lambda[", 1:ncol(W), "]"))

# Get estimated and generating values for wanted parameters
generating_values = c(alpha, beta, kappa, lambda)
sim_monitor <- monitor(sim_fit, probs = c(.025, .975), print = FALSE)
estimated_values <- sim_monitor[wanted_pars, c("mean", "2.5%", "97.5%")]

# Assesmble a data frame to pass to ggplot()
sim_df <- data.frame(parameter = factor(wanted_pars, rev(wanted_pars)),
                     row.names = NULL)
sim_df$middle <- estimated_values[,"mean"] - generating_values
sim_df$lower <- estimated_values[,"2.5%"] - generating_values
sim_df$upper <- estimated_values[,"97.5%"] - generating_values

# Plot the discrepancy
ggplot(sim_df) +
  aes(x = parameter, y = middle, ymin = lower, ymax = upper) +
  scale_x_discrete() +
  labs(y = "Discrepancy", x = NULL) +
  geom_abline(intercept = 0, slope = 0, color = "white") +
  geom_linerange() +
  geom_point(size = 2) +
  theme(panel.grid = element_blank()) +
  coord_flip()
```


# Example application

The example data are from the Consumer Protection and Perceptions of Science and Technology section of the 1992 Euro-Barometer Survey [@Karlheinz1992]. Because these data do not include person covariates, the latent regression aspect of the model will include an intercept only.

```{r example_data}
# Attach the example dataset. The ltm package is required.
data(Science, package = "ltm")

# Convert dataset to an integer matrix with values 0 ... 3
M <- matrix(NA, ncol = ncol(Science), nrow = nrow(Science))
for(i in 1:ncol(M)) M[, i] <- as.integer(Science[, i]) - 1
```

The dataset contains `r ncol(Science)` items and `r nrow(Science)` persons with no missing responses. The items pertain to attitudes towards science and technology, and responses are scored on a 4-point Likert scale. For example, the text of the first item reads, "Science and technology are making our lives healthier, easier and more comfortable." The response options are *strongly disagree*, *disagree*, *agree*, and *strongly agree*.

Before fitting the model, the response frequencies for each item are considered.

```{r example_freqs}
# Frequencies for each item
freqs <- t(apply(M, 2, table))
rownames(freqs) <- names(Science)
freqs
```

The data are now formatted into a list and fit with **Stan**.

```{r example_fit, results='hide'}
# Assemble data list for Stan
ex_list <- list(I = ncol(M),
                J = nrow(M),
                N = length(M),
                ii = rep(1:ncol(M), each = nrow(M)),
                jj = rep(1:nrow(M), times = ncol(M)),
                y = as.vector(M),
                K = 1,
                W = matrix(1, nrow = nrow(M), ncol = 1))

# Run Stan model
ex_fit <- stan(file = "grsm_latent_reg.stan",
               data = ex_list, chains = 4, iter = 1000)
```

As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior density, using $\hat{R}$.

```{r ex_converge, fig.cap="Convergence statistics ($\\hat{R}$) by parameter for the example. All values should be less than 1.1 to infer convergence."}
# Plot of convergence statistics
ex_monitor <- as.data.frame(monitor(ex_fit, print = FALSE))
ex_monitor$Parameter <- as.factor(gsub("\\[.*]", "", rownames(ex_monitor)))
ggplot(ex_monitor) +
  aes(x = Parameter, y = Rhat, color = Parameter) +
  geom_jitter(height = 0, width = .5, show.legend = FALSE) +
  ylab(expression(hat(italic(R))))
```

Next we view a summary of the parameter posteriors.

```{r example_print}
# View table of parameter posteriors
print(ex_fit, pars = c("alpha", "beta", "kappa", "lambda"))
```


# References

<!-- This comment causes section to be numbered -->
