---
title: "Partial credit model with latent regression"
author: "Daniel C. Furr"
date: "`r gsub('(^.* )(0)([0-9], .*)', '\\1\\3', format(Sys.time(), '%B %d, %Y'))`"
output:
  html_document:
    toc: true
    number_sections: true
    fig_caption: true
    css: ../styles.css
bibliography: ../bibliography.bib
---

```{r knitr_opts, include = FALSE}
knitr::opts_chunk$set(tidy = TRUE, cache = TRUE)
```


# Model

## Overview

The partial credit model [@masters1982rasch] is appropriate for item response data that features more than two *ordered* response categories for some or all items. The items may have differing numbers of response categories. For dichotomous items (items with exactly two response categories), the partical credit model is equivalent to the Rasch model. The version presented includes a latent regression. However, the latent regression part of the model may be restricted to an intercept only, resulting in the standard partial credit model.

$$
\Pr(Y_{ij} = y,~y > 0 | \theta_j, \beta_i) =
\frac{\exp \sum_{s=1}^y (\theta_j - \beta_{is})}
     {1 + \sum_{k=1}^{m_i} \exp \sum_{s=1}^k (\theta_j - \beta_{is})}
$$
$$
\Pr(Y_{ij} = y,~y = 0 | \theta_j, \beta_i) =
\frac{1}
     {1 + \sum_{k=1}^{m_i} \exp \sum_{s=1}^k (\theta_j - \beta_{is})}
$$
$$
\theta_j \sim \mathrm{N}(w_{j}' \lambda, \sigma^2)
$$

Variables:

* $i = 1 \ldots I$ indexes items.
* $j = 1 \ldots J$ indexes persons.
* $Y_{ij} \in \{ 0 \ldots m_i \}$ is the response of person $j$ to item $i$
* $m_i$ is simulataneously the maximum score and number of step difficulty parameters for item $i$.
* $w_{j}$ is the vector of covariates for person $j$, which will in general include an intercept term.

Parameters:

* $\beta_{is}$ is the $s$-th step difficulty for item $i$.
* $\theta_j$ is the ability for person $j$.
* $\lambda$ is the vector of latent regression parameters.
* $\sigma^2$ is the variance for the ability distribution.

Constraints:

* The last difficulty parameter is constrained to be the negative sum of the other difficulties, resulting in the average difficulty parameter being zero.

Priors:

* $\beta_1 \ldots \beta_{I-1} \sim \mathrm{N}(0, 5)$ is weakly informative, and no prior is needed for the contrained difficulty $\beta_I$.
* $\sigma \sim \mathrm{Exp}(.1)$ is weakly informative.
* A uniform prior is chosen for $\lambda$ because the scale of the person covariates will vary depending on the application.


## **Stan** program

A few aspects of the **Stan** program for the partial credit model bear mentioning. First, the program begins with the creation of a user-specified function `pcm_probs()`, which accepts a value for `theta` and a vector of parameters `beta` for one item. With these inputs, it returns a vector of model-predicted probabilities for each possible response. Later, in the model block, `pcm_probs()` is used to get the likelihood of the observed item responses.

Second, the encoding of item responses are modified such that the lowest response category is one instead of zero. This modification takes place in the transformed data block, in which a new variable `r` is created for this purpose. The adjustment is necessary for compatibility with the `categorical()` function.

Third, the variables `m` and `pos` are also created in the transformed data block. These are needed to pick out the vector of item parameters for a single item from the vector of all item parameters `beta`. `pos` indicates the position of the first parameter for a given item, while `m` indicates the count of parameters for an item. 

Lastly, a constraint is placed on the item difficulties for model identification. This is accomplished by creating `beta_free`, which is the vector of unconstrained item parameters, in the parameters block. Then in the transformed parameters block, `beta` is created to be identical to `beta_free` except for one additional element that is the constrained item difficulty. As a result of this constraint, the mean of `beta` will be zero.

```{r stan_code, comment="", echo=FALSE}
cat(readLines("pcm_latent_reg.stan"), sep = "\n")
```


# Simulation

First, the necessary **R** packages are loaded.

```{r, message=FALSE, warning=FALSE, results="hide"}
# Load R packages
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(ggplot2)
```

The **R** code that follows simulates a dataset conforming to the model. The **Stan** model will be evaluated in terms of its ability to recover the generating values of the parameters when fit to this dataset.

```{r sim_data}
# Generate person-related parameters and predictors
J <- 500
sigma <- 1
lambda <- c(.5, .5, .5)
W <- cbind(1, rnorm(J, 0, 1), rnorm(J, 0, 1))
theta <-  W %*% matrix(lambda) + rnorm(J, 0, sigma)

# Generate item parameters
I <- 20
Beta_uncentered <- matrix(NA, nrow = I, ncol = 2)
Beta_uncentered[,1] <- seq(from = -1.5, to = 1.5, length.out = I)
Beta_uncentered[,2] <- Beta_uncentered[,1] + rep(c(.25, .5, .75, 1), length.out = I)
Beta_centered <- Beta_uncentered - mean(Beta_uncentered)

# A function to simulate responses from the model
simulate_response <- function(theta, beta) {
  unsummed <- c(0, theta - beta)
  numerators <- exp(cumsum(unsummed))
  denominator <- sum(numerators)
  response_probs <- numerators/denominator
  simulated_y <- sample(1:length(response_probs) - 1, size = 1, 
                        prob = response_probs) 
  return(simulated_y)
}

# Assemble the data list to pass to Stan
data_list <- list(I = I,
                  J = J,
                  N = I*J,
                  ii = rep(1:I, times = J),
                  jj = rep(1:J, each = I))
data_list$y <- numeric(data_list$N)
for(n in 1:data_list$N) {
  data_list$y[n] <- simulate_response(theta[data_list$jj[n]], 
                                      Beta_centered[data_list$ii[n], ])
}
data_list$K <- ncol(W)
data_list$W <- W
```

The simulated data consists of `r I` items, each with `r ncol(Beta_centered) + 1` response categories, and `r J` persons. The simulated dataset is next fit with **Stan**.

```{r sim_fit, result="hide"}
#Fit model to simulated data
sim_fit <- stan(file = "pcm_latent_reg.stan", 
                data = data_list, chains = 4, iter = 200)
```

Before interpreting the results, it is necessary to check that the chains have converged. **Stan** provides the $\hat{R}$ statistic for the model parameters and log posterior. These are provided in the following figure. All values for $\hat{R}$ should be less than 1.1.

```{r sim_converge, fig.cap="Convergence statistics ($\\hat{R}$) by parameter for the simulation. All values should be less than 1.1 to infer convergence."}
# Plot of convergence statistics
sim_summary <- as.data.frame(summary(sim_fit)[[1]])
sim_summary$Parameter <- as.factor(gsub("\\[.*]", "", rownames(sim_summary)))
ggplot(sim_summary) +
  aes(x = Parameter, y = Rhat, color = Parameter) +
  geom_jitter(height = 0, width = .5, show.legend = FALSE) +
  ylab(expression(hat(italic(R))))
```

The **Stan** model is evaluated in terms of its ability to recover the generating values of the parameters. The R code below prepares a plot in which the points indicate the difference between the posterior means and generating values for the parameters of main interest. This difference is referred to as discrepancy. The lines indicate the 95% posterior intervals for the difference. Ideally, (nearly) all the 95% posterior intervals would include zero.

```{r sim_plot, fig.height=8, fig.cap="Discrepancies between estimated and generating parameters. Points indicate the difference between the posterior means and generating values for a parameter, and horizontal lines indicate 95% posterior intervals for the difference. Most of the discrepancies are about zero, indicating that **Stan** successfully recovers the true parameters."}
# Make vector of wanted parameter names
beta <- as.vector(t(Beta_centered))
wanted_pars <- c(paste0("beta[", 1:length(beta), "]"), 
                 paste0("lambda[", 1:ncol(W), "]"), 
                 "sigma")

# Get estimated and generating values for wanted parameters
generating_values = c(beta, lambda, sigma)
estimated_values <- sim_summary[wanted_pars, c("mean", "2.5%", "97.5%")]

# Assesmble a data frame to pass to ggplot()
sim_df <- data.frame(parameter = factor(wanted_pars, rev(wanted_pars)),
                     row.names = NULL)
sim_df$middle <- estimated_values[,"mean"] - generating_values
sim_df$lower <- estimated_values[,"2.5%"] - generating_values
sim_df$upper <- estimated_values[,"97.5%"] - generating_values

# Plot the discrepancy
ggplot(sim_df) +
  aes(x = parameter, y = middle, ymin = lower, ymax = upper) +
  scale_x_discrete() +
  labs(y = "Discrepancy", x = NULL) +
  geom_abline(intercept = 0, slope = 0, color = "white") +
  geom_linerange() +
  geom_point(size = 2) +
  theme(panel.grid = element_blank()) +
  coord_flip()
```


# Example application

The example data are from the TIMSS 2011 mathematics assessment [@mullis2012timss] of Australian and Taiwanese students. For convenience, a subset of 500 students is used. The subsetted data is then divided into a person covariate matrix and an item response matrix.

```{r example_subset_data}
# Attach the example dataset. The TAM package is required.
data(data.timssAusTwn.scored, package = "TAM")

# Subset the full data
select <- floor(seq(from = 1, to = nrow(data.timssAusTwn.scored), 
                    length.out = 500))
subsetted_df <- data.timssAusTwn.scored[select, ]
str(subsetted_df)
```


The dataset is next divided into an item response matrix and a matrix of student covariates.

```{r example_separate_matrices}
# Make a matrix of person predictors
w_mat <- cbind(intercept = rep(1, times = nrow(subsetted_df)),
               taiwan = as.numeric(subsetted_df$IDCNTRY == 158),
               female = as.numeric(subsetted_df$ITSEX == 2),
               book14 = as.numeric(subsetted_df$IDBOOK == 14))
head(w_mat)

# Make a matrix of item responses
y_mat <- as.matrix(subsetted_df[, grep("^M", names(subsetted_df))])
head(y_mat)
```

The person covariate matrix `w_mat` has columns representing an intercept and three indicator variables for being in Taiwan (versus Australia), being female (versus male), and being assigned test booklet 14 (instead of booklet 1). The item response matrix `y_mat` contains `r ncol(y_mat)` items. Neither the response matrix or person covariates contain missing data.

Before fitting the model, some descriptive statistics are considered. First, a frequency table for the person covariates is created.

```{r example_descriptives_1}
# Customized version of table() that does not omit missing categories. Specify
# expected categories with key.
consec_table <- function(x, key) {
  y <- rep(0, times = length(key))
  names(y) <- as.character(key)
  x_table <- table(x)
  y[names(x_table)] <- x_table
  return(y)
}

# Frequency table for person covariates
t(apply(w_mat, 2, consec_table, key = 0:1))
```

Next, a frequency table for item responses is considered.

```{r example_descriptives_2}
# Frequency table for item responses
item_freqs <- t(apply(y_mat, 2, consec_table, key = 0:2))
item_freqs
```

The table shows that the data are a mixture of dichotomous item and polytomous items with three responses categories. The first and second items are dichotomous, while the third and fourth are polytomous, for example. Consequently, the first and second items will have one step parameter each, while the third and fourth will have two each. *If* this table indicated there were no 0 or 1 responses for one of the polytomous items, responses for that item would need to be recoded.

Because all item parameters are stored in one vector, `beta`, some care is required in understanding which elements of `beta` correspond to which items. The following **R** code produces a table that maps each element in `beta` corresponds to its associated item and step.

```{r example_beta_key}
# Make a table mapping elements of beta to items/steps
x <- item_freqs[,-1] > 0
beta_key_trans <- t(matrix(NA, ncol = ncol(x), nrow = nrow(x)))
beta_key_trans[t(x[])] <- 1:sum(x)
beta_key <- t(beta_key_trans)
rownames(beta_key) <- rownames(item_freqs)
colnames(beta_key) <- paste("Step", 1:ncol(beta_key))
beta_key
```

The data are now formatted into a list and fit with **Stan**.

```{r example_fit, result="hide"}
# Assemble data list for Stan
ex_list <- list(I = ncol(y_mat),
                J = nrow(y_mat),
                N = length(y_mat),
                ii = rep(1:ncol(y_mat), each = nrow(y_mat)),
                jj = rep(1:nrow(y_mat), times = ncol(y_mat)),
                y = as.vector(y_mat),
                K = ncol(w_mat),
                W = w_mat)

# Run Stan model
ex_fit <- stan(file = "pcm_latent_reg.stan",
               data = ex_list, chains = 4, iter = 200)
```

As discussed above, convergence of the chains is assessed for every parameter, and also the log posterior density, using $\hat{R}$.

```{r ex_converge, fig.cap="Convergence statistics ($\\hat{R}$) by parameter for the example. All values should be less than 1.1 to infer convergence."}
# Plot of convergence statistics
ex_summary <- as.data.frame(summary(ex_fit)[[1]])
ex_summary$Parameter <- as.factor(gsub("\\[.*]", "", rownames(ex_summary)))
ggplot(ex_summary) +
  aes(x = Parameter, y = Rhat, color = Parameter) +
  geom_jitter(height = 0, width = .5, show.legend = FALSE) +
  ylab(expression(hat(italic(R))))
```

Next we view a summary of the parameter posteriors.

```{r example_print}
# View table of parameter posteriors
print(ex_fit, pars = c("beta", "lambda", "sigma"))
```

If person covariates are unavailable, or their inclusion unwanted, the model may be fit restricting the matrix of person covariates to an intercept only. In this case, the vector `lambda` contains only one element, which will represent the mean of the ability distribution. The code below is an example of how to structure the data for **Stan** for this purpose.

```{r example_noreg, eval=FALSE}
# Fit the example data without latent regression
noreg_list <- ex_list
noreg_list$K <- 1
noreg_list$W <- matrix(1, nrow = nrow(M), ncol = 1)
noreg_fit <- stan(file = "pcm_latent_reg.stan",
                data = noreg_list, chains = 4, iter = 200)
```


# References

<!-- This comment causes section to be numbered -->
